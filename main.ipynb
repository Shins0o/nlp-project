{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset du prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Spécifiez le chemin du dossier contenant les fichiers xlsx\n",
    "dossier_xlsx = 'Traduction avis clients'\n",
    "\n",
    "# Initialisez un DataFrame vide\n",
    "data_frame_global = pd.DataFrame()\n",
    "\n",
    "# Parcourez tous les fichiers dans le dossier\n",
    "for fichier in os.listdir(dossier_xlsx):\n",
    "    if fichier.endswith('.xlsx'):\n",
    "        chemin_fichier = os.path.join(dossier_xlsx, fichier)\n",
    "        \n",
    "        # Lisez le fichier Excel et ajoutez les données au DataFrame global\n",
    "        df_fichier = pd.read_excel(chemin_fichier)\n",
    "        data_frame_global = pd.concat([data_frame_global, df_fichier], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "      <th>auteur</th>\n",
       "      <th>avis</th>\n",
       "      <th>assureur</th>\n",
       "      <th>produit</th>\n",
       "      <th>type</th>\n",
       "      <th>date_publication</th>\n",
       "      <th>date_exp</th>\n",
       "      <th>avis_en</th>\n",
       "      <th>avis_cor</th>\n",
       "      <th>avis_cor_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>croco47-108902</td>\n",
       "      <td>nouvelle adhérente et ayant été en contact ave...</td>\n",
       "      <td>Néoliane Santé</td>\n",
       "      <td>sante</td>\n",
       "      <td>train</td>\n",
       "      <td>02/04/2021</td>\n",
       "      <td>01/04/2021</td>\n",
       "      <td>New member and having been in contact with Pop...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dt-117016</td>\n",
       "      <td>Service téléphonique lamentable, impossibilité...</td>\n",
       "      <td>Malakoff Humanis</td>\n",
       "      <td>prevoyance</td>\n",
       "      <td>train</td>\n",
       "      <td>14/06/2021</td>\n",
       "      <td>01/06/2021</td>\n",
       "      <td>Latable telephone service, impossibility of ha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>william-g-130367</td>\n",
       "      <td>Je suis très satisfait de l'ensemble des prest...</td>\n",
       "      <td>GMF</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>31/08/2021</td>\n",
       "      <td>01/08/2021</td>\n",
       "      <td>I am very satisfied with all the services offe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>gilberte-c-138206</td>\n",
       "      <td>Je suis satisfaite le prix me convient très bi...</td>\n",
       "      <td>APRIL</td>\n",
       "      <td>sante</td>\n",
       "      <td>train</td>\n",
       "      <td>25/10/2021</td>\n",
       "      <td>01/10/2021</td>\n",
       "      <td>I am satisfied the price suits me very well I ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mica-72266</td>\n",
       "      <td>..l.... .  Nickel pour l instant,  pas de  sin...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>18/03/2019</td>\n",
       "      <td>01/03/2019</td>\n",
       "      <td>..I.... . Nickel for the moment, no sinister w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3</td>\n",
       "      <td>patrice-63215</td>\n",
       "      <td>POUR LE MOMENT JE SUIS SATISFAIT DE CETTE MUTU...</td>\n",
       "      <td>Santiane</td>\n",
       "      <td>sante</td>\n",
       "      <td>train</td>\n",
       "      <td>12/04/2018</td>\n",
       "      <td>01/04/2018</td>\n",
       "      <td>For the moment I am satisfied with this mutual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5</td>\n",
       "      <td>benjamin-d-111262</td>\n",
       "      <td>Le service et l'accompagnement sont de très bo...</td>\n",
       "      <td>Zen'Up</td>\n",
       "      <td>credit</td>\n",
       "      <td>train</td>\n",
       "      <td>22/04/2021</td>\n",
       "      <td>01/04/2021</td>\n",
       "      <td>The service and support are of very good quali...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>geal-136804</td>\n",
       "      <td>La pire assurance dans laquelle j'ai été.\\nDou...</td>\n",
       "      <td>Active Assurances</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>09/10/2021</td>\n",
       "      <td>01/10/2021</td>\n",
       "      <td>The worst insurance in which I was.\\nDoubly of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>3</td>\n",
       "      <td>castanho-p-108077</td>\n",
       "      <td>Je suis satisfe. Le prix ça me convien. Pas ch...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>26/03/2021</td>\n",
       "      <td>01/03/2021</td>\n",
       "      <td>I am satisfied. The price suits me. Cheap. The...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>5</td>\n",
       "      <td>kathalina--p-135739</td>\n",
       "      <td>Très bien, bonne rapidité je recommande viveme...</td>\n",
       "      <td>APRIL Moto</td>\n",
       "      <td>moto</td>\n",
       "      <td>train</td>\n",
       "      <td>03/10/2021</td>\n",
       "      <td>01/10/2021</td>\n",
       "      <td>Very well, good speed I highly recommend, effi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     note               auteur  \\\n",
       "0       4       croco47-108902   \n",
       "1       1            dt-117016   \n",
       "2       5     william-g-130367   \n",
       "3       4    gilberte-c-138206   \n",
       "4       4           mica-72266   \n",
       "..    ...                  ...   \n",
       "995     3        patrice-63215   \n",
       "996     5    benjamin-d-111262   \n",
       "997     1          geal-136804   \n",
       "998     3    castanho-p-108077   \n",
       "999     5  kathalina--p-135739   \n",
       "\n",
       "                                                  avis             assureur  \\\n",
       "0    nouvelle adhérente et ayant été en contact ave...       Néoliane Santé   \n",
       "1    Service téléphonique lamentable, impossibilité...     Malakoff Humanis   \n",
       "2    Je suis très satisfait de l'ensemble des prest...                  GMF   \n",
       "3    Je suis satisfaite le prix me convient très bi...                APRIL   \n",
       "4    ..l.... .  Nickel pour l instant,  pas de  sin...  L'olivier Assurance   \n",
       "..                                                 ...                  ...   \n",
       "995  POUR LE MOMENT JE SUIS SATISFAIT DE CETTE MUTU...             Santiane   \n",
       "996  Le service et l'accompagnement sont de très bo...               Zen'Up   \n",
       "997  La pire assurance dans laquelle j'ai été.\\nDou...    Active Assurances   \n",
       "998  Je suis satisfe. Le prix ça me convien. Pas ch...  L'olivier Assurance   \n",
       "999  Très bien, bonne rapidité je recommande viveme...           APRIL Moto   \n",
       "\n",
       "        produit   type date_publication    date_exp  \\\n",
       "0         sante  train       02/04/2021  01/04/2021   \n",
       "1    prevoyance  train       14/06/2021  01/06/2021   \n",
       "2          auto  train       31/08/2021  01/08/2021   \n",
       "3         sante  train       25/10/2021  01/10/2021   \n",
       "4          auto  train       18/03/2019  01/03/2019   \n",
       "..          ...    ...              ...         ...   \n",
       "995       sante  train       12/04/2018  01/04/2018   \n",
       "996      credit  train       22/04/2021  01/04/2021   \n",
       "997        auto  train       09/10/2021  01/10/2021   \n",
       "998        auto  train       26/03/2021  01/03/2021   \n",
       "999        moto  train       03/10/2021  01/10/2021   \n",
       "\n",
       "                                               avis_en  avis_cor  avis_cor_en  \n",
       "0    New member and having been in contact with Pop...       NaN          NaN  \n",
       "1    Latable telephone service, impossibility of ha...       NaN          NaN  \n",
       "2    I am very satisfied with all the services offe...       NaN          NaN  \n",
       "3    I am satisfied the price suits me very well I ...       NaN          NaN  \n",
       "4    ..I.... . Nickel for the moment, no sinister w...       NaN          NaN  \n",
       "..                                                 ...       ...          ...  \n",
       "995     For the moment I am satisfied with this mutual       NaN          NaN  \n",
       "996  The service and support are of very good quali...       NaN          NaN  \n",
       "997  The worst insurance in which I was.\\nDoubly of...       NaN          NaN  \n",
       "998  I am satisfied. The price suits me. Cheap. The...       NaN          NaN  \n",
       "999  Very well, good speed I highly recommend, effi...       NaN          NaN  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_frame_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "      <th>auteur</th>\n",
       "      <th>avis</th>\n",
       "      <th>assureur</th>\n",
       "      <th>produit</th>\n",
       "      <th>type</th>\n",
       "      <th>date_publication</th>\n",
       "      <th>date_exp</th>\n",
       "      <th>avis_en</th>\n",
       "      <th>avis_cor</th>\n",
       "      <th>avis_cor_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>audurier-c-136272</td>\n",
       "      <td>La personne au téléphone était Clair et sympat...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>06/10/2021</td>\n",
       "      <td>01/10/2021</td>\n",
       "      <td>The person on the phone was clear and friendly...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>paul-a-122970</td>\n",
       "      <td>Satisfait.\\n\\nRéactivité, simplicité. Prix att...</td>\n",
       "      <td>APRIL Moto</td>\n",
       "      <td>moto</td>\n",
       "      <td>train</td>\n",
       "      <td>09/07/2021</td>\n",
       "      <td>01/07/2021</td>\n",
       "      <td>Satisfied.\\n\\nReactivity, simplicity. Attracti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>kitty-38517</td>\n",
       "      <td>Assureur à fuir, n assure pas ses responsabili...</td>\n",
       "      <td>SwissLife</td>\n",
       "      <td>vie</td>\n",
       "      <td>train</td>\n",
       "      <td>15/10/2020</td>\n",
       "      <td>01/10/2020</td>\n",
       "      <td>Insurer to flee, does not ensure its responsib...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>laure97134-87907</td>\n",
       "      <td>Voilà 3 mois que la GMF me fait attendre pour ...</td>\n",
       "      <td>GMF</td>\n",
       "      <td>habitation</td>\n",
       "      <td>train</td>\n",
       "      <td>03/03/2020</td>\n",
       "      <td>01/03/2020</td>\n",
       "      <td>The GMF has been waiting for a water damage fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>bourouane-l-129916</td>\n",
       "      <td>Je suis bien avec cet assurance.elle est prati...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>28/08/2021</td>\n",
       "      <td>01/08/2021</td>\n",
       "      <td>I am good with this insurance. She is practica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34430</th>\n",
       "      <td>3.0</td>\n",
       "      <td>patrice-63215</td>\n",
       "      <td>POUR LE MOMENT JE SUIS SATISFAIT DE CETTE MUTU...</td>\n",
       "      <td>Santiane</td>\n",
       "      <td>sante</td>\n",
       "      <td>train</td>\n",
       "      <td>12/04/2018</td>\n",
       "      <td>01/04/2018</td>\n",
       "      <td>For the moment I am satisfied with this mutual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34431</th>\n",
       "      <td>5.0</td>\n",
       "      <td>benjamin-d-111262</td>\n",
       "      <td>Le service et l'accompagnement sont de très bo...</td>\n",
       "      <td>Zen'Up</td>\n",
       "      <td>credit</td>\n",
       "      <td>train</td>\n",
       "      <td>22/04/2021</td>\n",
       "      <td>01/04/2021</td>\n",
       "      <td>The service and support are of very good quali...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34432</th>\n",
       "      <td>1.0</td>\n",
       "      <td>geal-136804</td>\n",
       "      <td>La pire assurance dans laquelle j'ai été.\\nDou...</td>\n",
       "      <td>Active Assurances</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>09/10/2021</td>\n",
       "      <td>01/10/2021</td>\n",
       "      <td>The worst insurance in which I was.\\nDoubly of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34433</th>\n",
       "      <td>3.0</td>\n",
       "      <td>castanho-p-108077</td>\n",
       "      <td>Je suis satisfe. Le prix ça me convien. Pas ch...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>26/03/2021</td>\n",
       "      <td>01/03/2021</td>\n",
       "      <td>I am satisfied. The price suits me. Cheap. The...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34434</th>\n",
       "      <td>5.0</td>\n",
       "      <td>kathalina--p-135739</td>\n",
       "      <td>Très bien, bonne rapidité je recommande viveme...</td>\n",
       "      <td>APRIL Moto</td>\n",
       "      <td>moto</td>\n",
       "      <td>train</td>\n",
       "      <td>03/10/2021</td>\n",
       "      <td>01/10/2021</td>\n",
       "      <td>Very well, good speed I highly recommend, effi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34435 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       note               auteur  \\\n",
       "0       4.0    audurier-c-136272   \n",
       "1       4.0        paul-a-122970   \n",
       "2       1.0          kitty-38517   \n",
       "3       1.0     laure97134-87907   \n",
       "4       3.0   bourouane-l-129916   \n",
       "...     ...                  ...   \n",
       "34430   3.0        patrice-63215   \n",
       "34431   5.0    benjamin-d-111262   \n",
       "34432   1.0          geal-136804   \n",
       "34433   3.0    castanho-p-108077   \n",
       "34434   5.0  kathalina--p-135739   \n",
       "\n",
       "                                                    avis             assureur  \\\n",
       "0      La personne au téléphone était Clair et sympat...  L'olivier Assurance   \n",
       "1      Satisfait.\\n\\nRéactivité, simplicité. Prix att...           APRIL Moto   \n",
       "2      Assureur à fuir, n assure pas ses responsabili...            SwissLife   \n",
       "3      Voilà 3 mois que la GMF me fait attendre pour ...                  GMF   \n",
       "4      Je suis bien avec cet assurance.elle est prati...  L'olivier Assurance   \n",
       "...                                                  ...                  ...   \n",
       "34430  POUR LE MOMENT JE SUIS SATISFAIT DE CETTE MUTU...             Santiane   \n",
       "34431  Le service et l'accompagnement sont de très bo...               Zen'Up   \n",
       "34432  La pire assurance dans laquelle j'ai été.\\nDou...    Active Assurances   \n",
       "34433  Je suis satisfe. Le prix ça me convien. Pas ch...  L'olivier Assurance   \n",
       "34434  Très bien, bonne rapidité je recommande viveme...           APRIL Moto   \n",
       "\n",
       "          produit   type date_publication    date_exp  \\\n",
       "0            auto  train       06/10/2021  01/10/2021   \n",
       "1            moto  train       09/07/2021  01/07/2021   \n",
       "2             vie  train       15/10/2020  01/10/2020   \n",
       "3      habitation  train       03/03/2020  01/03/2020   \n",
       "4            auto  train       28/08/2021  01/08/2021   \n",
       "...           ...    ...              ...         ...   \n",
       "34430       sante  train       12/04/2018  01/04/2018   \n",
       "34431      credit  train       22/04/2021  01/04/2021   \n",
       "34432        auto  train       09/10/2021  01/10/2021   \n",
       "34433        auto  train       26/03/2021  01/03/2021   \n",
       "34434        moto  train       03/10/2021  01/10/2021   \n",
       "\n",
       "                                                 avis_en avis_cor avis_cor_en  \n",
       "0      The person on the phone was clear and friendly...      NaN         NaN  \n",
       "1      Satisfied.\\n\\nReactivity, simplicity. Attracti...      NaN         NaN  \n",
       "2      Insurer to flee, does not ensure its responsib...      NaN         NaN  \n",
       "3      The GMF has been waiting for a water damage fo...      NaN         NaN  \n",
       "4      I am good with this insurance. She is practica...      NaN         NaN  \n",
       "...                                                  ...      ...         ...  \n",
       "34430     For the moment I am satisfied with this mutual      NaN         NaN  \n",
       "34431  The service and support are of very good quali...      NaN         NaN  \n",
       "34432  The worst insurance in which I was.\\nDoubly of...      NaN         NaN  \n",
       "34433  I am satisfied. The price suits me. Cheap. The...      NaN         NaN  \n",
       "34434  Very well, good speed I highly recommend, effi...      NaN         NaN  \n",
       "\n",
       "[34435 rows x 11 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['avis_cor', 'avis_cor_en'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "note                10331\n",
       "auteur                  1\n",
       "avis                    0\n",
       "assureur                0\n",
       "produit                 0\n",
       "type                    0\n",
       "date_publication        0\n",
       "date_exp                0\n",
       "avis_en                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['avis_en'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = ['avis_en'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eddin\\AppData\\Local\\Temp\\ipykernel_2204\\1969485904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sub['avis_en_corrige'] = df_sub['avis_en'].apply(correction_orthographique)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note</th>\n",
       "      <th>auteur</th>\n",
       "      <th>avis</th>\n",
       "      <th>assureur</th>\n",
       "      <th>produit</th>\n",
       "      <th>type</th>\n",
       "      <th>date_publication</th>\n",
       "      <th>date_exp</th>\n",
       "      <th>avis_en</th>\n",
       "      <th>avis_en_corrige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>audurier-c-136272</td>\n",
       "      <td>La personne au téléphone était Clair et sympat...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>06/10/2021</td>\n",
       "      <td>01/10/2021</td>\n",
       "      <td>The person on the phone was clear and friendly...</td>\n",
       "      <td>The person on the phone was clear and friendly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>paul-a-122970</td>\n",
       "      <td>Satisfait.\\n\\nRéactivité, simplicité. Prix att...</td>\n",
       "      <td>APRIL Moto</td>\n",
       "      <td>moto</td>\n",
       "      <td>train</td>\n",
       "      <td>09/07/2021</td>\n",
       "      <td>01/07/2021</td>\n",
       "      <td>Satisfied.\\n\\nReactivity, simplicity. Attracti...</td>\n",
       "      <td>Satisfied.\\n\\nActivity, simplicity. Attractive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>kitty-38517</td>\n",
       "      <td>Assureur à fuir, n assure pas ses responsabili...</td>\n",
       "      <td>SwissLife</td>\n",
       "      <td>vie</td>\n",
       "      <td>train</td>\n",
       "      <td>15/10/2020</td>\n",
       "      <td>01/10/2020</td>\n",
       "      <td>Insurer to flee, does not ensure its responsib...</td>\n",
       "      <td>Ensure to flee, does not ensure its responsibi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>laure97134-87907</td>\n",
       "      <td>Voilà 3 mois que la GMF me fait attendre pour ...</td>\n",
       "      <td>GMF</td>\n",
       "      <td>habitation</td>\n",
       "      <td>train</td>\n",
       "      <td>03/03/2020</td>\n",
       "      <td>01/03/2020</td>\n",
       "      <td>The GMF has been waiting for a water damage fo...</td>\n",
       "      <td>The GMF has been waiting for a water damage fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>bourouane-l-129916</td>\n",
       "      <td>Je suis bien avec cet assurance.elle est prati...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>28/08/2021</td>\n",
       "      <td>01/08/2021</td>\n",
       "      <td>I am good with this insurance. She is practica...</td>\n",
       "      <td>I am good with this insurance. The is practica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.0</td>\n",
       "      <td>leovisolbla-60835</td>\n",
       "      <td>Pour le moment rien à dire,\\nle service client...</td>\n",
       "      <td>L'olivier Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>26/01/2018</td>\n",
       "      <td>01/01/2018</td>\n",
       "      <td>For the moment nothing to say,\\nCustomer servi...</td>\n",
       "      <td>For the moment nothing to say,\\nCustomer servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.0</td>\n",
       "      <td>antoine-o-123973</td>\n",
       "      <td>bon services et conseils. accueil téléphonique...</td>\n",
       "      <td>GMF</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>20/07/2021</td>\n",
       "      <td>01/07/2021</td>\n",
       "      <td>Good services and advice. Home telephone good,...</td>\n",
       "      <td>Good services and advice. Some telephone good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>isam-117713</td>\n",
       "      <td>faire très attention à la résiliation par un a...</td>\n",
       "      <td>Néoliane Santé</td>\n",
       "      <td>sante</td>\n",
       "      <td>train</td>\n",
       "      <td>21/06/2021</td>\n",
       "      <td>01/06/2021</td>\n",
       "      <td>Pay attention to the termination by another br...</td>\n",
       "      <td>May attention to the termination by another br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5.0</td>\n",
       "      <td>louxor-122493</td>\n",
       "      <td>Tarif compétitif, service client réactif et in...</td>\n",
       "      <td>AMV</td>\n",
       "      <td>moto</td>\n",
       "      <td>train</td>\n",
       "      <td>06/07/2021</td>\n",
       "      <td>01/07/2021</td>\n",
       "      <td>Competitive price, reactive customer service a...</td>\n",
       "      <td>Competitive price, reactive customer service a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4.0</td>\n",
       "      <td>chantal-v-114690</td>\n",
       "      <td>Je suis satisfaite du service et des prix mais...</td>\n",
       "      <td>Direct Assurance</td>\n",
       "      <td>auto</td>\n",
       "      <td>train</td>\n",
       "      <td>24/05/2021</td>\n",
       "      <td>01/05/2021</td>\n",
       "      <td>I am satisfied with the service and the prices...</td>\n",
       "      <td>I am satisfied with the service and the prices...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    note              auteur  \\\n",
       "0    4.0   audurier-c-136272   \n",
       "1    4.0       paul-a-122970   \n",
       "2    1.0         kitty-38517   \n",
       "3    1.0    laure97134-87907   \n",
       "4    3.0  bourouane-l-129916   \n",
       "..   ...                 ...   \n",
       "95   5.0   leovisolbla-60835   \n",
       "96   5.0    antoine-o-123973   \n",
       "97   1.0         isam-117713   \n",
       "98   5.0       louxor-122493   \n",
       "99   4.0    chantal-v-114690   \n",
       "\n",
       "                                                 avis             assureur  \\\n",
       "0   La personne au téléphone était Clair et sympat...  L'olivier Assurance   \n",
       "1   Satisfait.\\n\\nRéactivité, simplicité. Prix att...           APRIL Moto   \n",
       "2   Assureur à fuir, n assure pas ses responsabili...            SwissLife   \n",
       "3   Voilà 3 mois que la GMF me fait attendre pour ...                  GMF   \n",
       "4   Je suis bien avec cet assurance.elle est prati...  L'olivier Assurance   \n",
       "..                                                ...                  ...   \n",
       "95  Pour le moment rien à dire,\\nle service client...  L'olivier Assurance   \n",
       "96  bon services et conseils. accueil téléphonique...                  GMF   \n",
       "97  faire très attention à la résiliation par un a...       Néoliane Santé   \n",
       "98  Tarif compétitif, service client réactif et in...                  AMV   \n",
       "99  Je suis satisfaite du service et des prix mais...     Direct Assurance   \n",
       "\n",
       "       produit   type date_publication    date_exp  \\\n",
       "0         auto  train       06/10/2021  01/10/2021   \n",
       "1         moto  train       09/07/2021  01/07/2021   \n",
       "2          vie  train       15/10/2020  01/10/2020   \n",
       "3   habitation  train       03/03/2020  01/03/2020   \n",
       "4         auto  train       28/08/2021  01/08/2021   \n",
       "..         ...    ...              ...         ...   \n",
       "95        auto  train       26/01/2018  01/01/2018   \n",
       "96        auto  train       20/07/2021  01/07/2021   \n",
       "97       sante  train       21/06/2021  01/06/2021   \n",
       "98        moto  train       06/07/2021  01/07/2021   \n",
       "99        auto  train       24/05/2021  01/05/2021   \n",
       "\n",
       "                                              avis_en  \\\n",
       "0   The person on the phone was clear and friendly...   \n",
       "1   Satisfied.\\n\\nReactivity, simplicity. Attracti...   \n",
       "2   Insurer to flee, does not ensure its responsib...   \n",
       "3   The GMF has been waiting for a water damage fo...   \n",
       "4   I am good with this insurance. She is practica...   \n",
       "..                                                ...   \n",
       "95  For the moment nothing to say,\\nCustomer servi...   \n",
       "96  Good services and advice. Home telephone good,...   \n",
       "97  Pay attention to the termination by another br...   \n",
       "98  Competitive price, reactive customer service a...   \n",
       "99  I am satisfied with the service and the prices...   \n",
       "\n",
       "                                      avis_en_corrige  \n",
       "0   The person on the phone was clear and friendly...  \n",
       "1   Satisfied.\\n\\nActivity, simplicity. Attractive...  \n",
       "2   Ensure to flee, does not ensure its responsibi...  \n",
       "3   The GMF has been waiting for a water damage fo...  \n",
       "4   I am good with this insurance. The is practica...  \n",
       "..                                                ...  \n",
       "95  For the moment nothing to say,\\nCustomer servi...  \n",
       "96  Good services and advice. Some telephone good,...  \n",
       "97  May attention to the termination by another br...  \n",
       "98  Competitive price, reactive customer service a...  \n",
       "99  I am satisfied with the service and the prices...  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correction_orthographique(texte):\n",
    "    blob = TextBlob(texte)\n",
    "    return str(blob.correct())\n",
    "\n",
    "# Appliquer la correction orthographique à la colonne \"avis_en\"\n",
    "df_sub['avis_en_corrige'] = df_sub['avis_en'].apply(correction_orthographique)\n",
    "\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eddin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist, ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots Unigrammes Fréquents:\n",
      "[('insurance', 40), ('service', 29), ('satisfied', 23), ('price', 21), ('contract', 21), ('customer', 17), ('years', 17), ('good', 16), ('without', 14), ('since', 14)]\n",
      "\n",
      "Bigrammes Fréquents:\n",
      "[('satisfied service', 7), ('customer service', 7), ('olive tree', 7), ('2 years', 4), ('value money', 4), ('3 months', 3), ('several times', 3), ('bank account', 3), ('good value', 3), ('highly recommend', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour extraire les n-grammes d'une chaîne de texte sans ponctuation et stop words\n",
    "def extraire_ngram(texte, n):\n",
    "    tokens = word_tokenize(texte)\n",
    "    tokens_sans_ponctuation = [mot for mot in tokens if mot.isalnum()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_sans_stopwords = [mot for mot in tokens_sans_ponctuation if mot.lower() not in stop_words]\n",
    "    n_grams = ngrams(tokens_sans_stopwords, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Appliquer la fonction d'extraction des n-grammes et calculer la fréquence des mots sans ponctuation et stop words\n",
    "avis_corrige = ' '.join(df_sub['avis_en_corrige'])\n",
    "unigram_freq = FreqDist([mot for mot in word_tokenize(avis_corrige) if mot.isalnum() and mot.lower() not in stopwords.words('english')])\n",
    "bigram_freq = FreqDist(extraire_ngram(avis_corrige, 2))\n",
    "\n",
    "# Afficher les mots unigrammes et bigrammes les plus fréquents\n",
    "print(\"Mots Unigrammes Fréquents:\")\n",
    "print(unigram_freq.most_common(10))\n",
    "\n",
    "print(\"\\nBigrammes Fréquents:\")\n",
    "print(bigram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp38-cp38-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\cyprien\\anaconda3\\lib\\site-packages (from gensim) (1.22.4)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "     ---------------------------------------- 42.2/42.2 MB 1.3 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of gensim to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 1.3 MB/s eta 0:00:00\n",
      "  Downloading gensim-4.3.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 1.7 MB/s eta 0:00:00\n",
      "  Downloading gensim-4.2.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\cyprien\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "  Downloading gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 1.4 MB/s eta 0:00:00\n",
      "  Downloading gensim-4.1.1-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 1.7 MB/s eta 0:00:00\n",
      "  Downloading gensim-4.1.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     ---------------------------------------- 24.0/24.0 MB 1.5 MB/s eta 0:00:00\n",
      "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
      "     ---------------------------------------- 23.9/23.9 MB 1.6 MB/s eta 0:00:00\n",
      "INFO: pip is still looking at multiple versions of gensim to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading gensim-4.0.0-cp38-cp38-win_amd64.whl (23.8 MB)\n",
      "     ---------------------------------------- 23.8/23.8 MB 1.7 MB/s eta 0:00:00\n",
      "  Downloading gensim-3.8.3-cp38-cp38-win_amd64.whl (24.2 MB)\n",
      "     ---------------------------------------- 24.2/24.2 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\cyprien\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "  Downloading gensim-3.8.2.tar.gz (23.4 MB)\n",
      "     ---------------------------------------- 23.4/23.4 MB 1.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n",
      "     ---------------------------------------- 23.4/23.4 MB 1.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.8.0.tar.gz (23.4 MB)\n",
      "     ---------------------------------------- 23.4/23.4 MB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading gensim-3.7.3.tar.gz (23.4 MB)\n",
      "     ---------------------------------------- 23.4/23.4 MB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.7.2.tar.gz (23.4 MB)\n",
      "     ---------------------------------------- 23.4/23.4 MB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.7.1.tar.gz (23.4 MB)\n",
      "     ---------------------------------------- 23.4/23.4 MB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.7.0.tar.gz (23.4 MB)\n",
      "     ---------------------------------------- 23.4/23.4 MB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n",
      "     ---------------------------------------- 23.1/23.1 MB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.5.0.tar.gz (22.9 MB)\n",
      "     ---------------------------------------- 22.9/22.9 MB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.4.0.tar.gz (22.2 MB)\n",
      "     ---------------------------------------- 22.2/22.2 MB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.3.0.tar.gz (21.9 MB)\n",
      "     ---------------------------------------- 21.9/21.9 MB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 95, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 546, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 427, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 237, in _attempt_to_pin_criterion\n",
      "    for candidate in criterion.candidates:\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 47, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 182, in _make_candidate_from_link\n",
      "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 228, in _make_base_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 293, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 156, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 225, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 304, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 525, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 596, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 168, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 109, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 147, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 587, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\Cyprien\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading gensim-3.2.0.tar.gz (15.3 MB)\n",
      "     ---------------------------------------- 15.3/15.3 MB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.1.0.tar.gz (15.1 MB)\n",
      "     ---------------------------------------- 15.1/15.1 MB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-3.0.0.tar.gz (59.5 MB)\n",
      "     ---------------------------------------- 59.5/59.5 MB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-2.3.0.tar.gz (17.2 MB)\n",
      "     ---------------------------------------- 17.2/17.2 MB 1.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-2.2.0.tar.gz (16.6 MB)\n",
      "     ---------------------------------------- 16.6/16.6 MB 1.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-2.1.0.tar.gz (15.1 MB)\n",
      "     ---------------------------------------- 15.1/15.1 MB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-2.0.0.tar.gz (14.1 MB)\n",
      "     ---------------------------------------- 14.1/14.1 MB 1.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading gensim-1.0.1.tar.gz (13.9 MB)\n",
      "     --------                                 2.9/13.9 MB 1.1 MB/s eta 0:00:10\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-231e5000bc0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msumy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msumy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlsa\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLsaSummarizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from nltk import FreqDist, ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyprien\\OneDrive - De Vinci\\Documents\\ESILV\\A5\\Machine_Learning_NLP\\Project2\\nlp-project\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\Cyprien\\OneDrive - De Vinci\\Documents\\ESILV\\A5\\Machine_Learning_NLP\\Project2\\nlp-project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Average Rating</th>\n",
       "      <th>Experience Date</th>\n",
       "      <th>Username</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Information</th>\n",
       "      <th>Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Buyagift</td>\n",
       "      <td>['Hotel', 'Adventure Sports', 'Racetrack', 'Co...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>Ms Cherith Adams</td>\n",
       "      <td>Amazing offer on a spa retreat…</td>\n",
       "      <td>Buyagift is the UK's leading provider of gift ...</td>\n",
       "      <td>I got an amazing offer on a spa retreat and th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Buyagift</td>\n",
       "      <td>['Hotel', 'Adventure Sports', 'Racetrack', 'Co...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>Mr Paul Parker</td>\n",
       "      <td>Bought vouchers for a family birthday…</td>\n",
       "      <td>Buyagift is the UK's leading provider of gift ...</td>\n",
       "      <td>Bought vouchers for a family birthday present ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Buyagift</td>\n",
       "      <td>['Hotel', 'Adventure Sports', 'Racetrack', 'Co...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>Gerri</td>\n",
       "      <td>Amazing Cream Tea Experience</td>\n",
       "      <td>Buyagift is the UK's leading provider of gift ...</td>\n",
       "      <td>A home delivery of our Cream Tea Experience fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Buyagift</td>\n",
       "      <td>['Hotel', 'Adventure Sports', 'Racetrack', 'Co...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>pPenny Thornton</td>\n",
       "      <td>My choice was not available but the BAG team  ...</td>\n",
       "      <td>Buyagift is the UK's leading provider of gift ...</td>\n",
       "      <td>My experience was not available when I tried t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Buyagift</td>\n",
       "      <td>['Hotel', 'Adventure Sports', 'Racetrack', 'Co...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>Adrienne Ralphs</td>\n",
       "      <td>I bought a tank drive for a 50th…</td>\n",
       "      <td>Buyagift is the UK's leading provider of gift ...</td>\n",
       "      <td>I bought a tank drive for a 50th birthday pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>5</td>\n",
       "      <td>Langstone Manor Holiday Park</td>\n",
       "      <td>['Camping Shop']</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2022-07-09</td>\n",
       "      <td>Liam Perry</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>Set amongst delightful mature grounds in a she...</td>\n",
       "      <td>We had a fantastic time here recently having u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3892</th>\n",
       "      <td>5</td>\n",
       "      <td>DickHartley</td>\n",
       "      <td>['Restaurant']</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2022-06-16</td>\n",
       "      <td>John</td>\n",
       "      <td>Good to do business with.</td>\n",
       "      <td>Dick Hartley is an independent supplier of kit...</td>\n",
       "      <td>Good to do business with.Bought some Riedel gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3893</th>\n",
       "      <td>5</td>\n",
       "      <td>The Beachfield Hotel</td>\n",
       "      <td>['Family Hotel']</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>Bad Experience.</td>\n",
       "      <td>Clean ,competitive pricing,great food…</td>\n",
       "      <td>Welcome to Beachfield Hotel. We're a boutique ...</td>\n",
       "      <td>Clean ,competitive pricing,great food and staf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3894</th>\n",
       "      <td>5</td>\n",
       "      <td>Crofton and Park</td>\n",
       "      <td>['Jewellery Repair Service']</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2023-10-25</td>\n",
       "      <td>Poppy Khaisee-Headley</td>\n",
       "      <td>Excellent and professional service</td>\n",
       "      <td>A premium,personal concierge service.\\r\\n\\r\\nA...</td>\n",
       "      <td>Excellent and professional service. I had a di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3895</th>\n",
       "      <td>5</td>\n",
       "      <td>Haute Dolci Watford</td>\n",
       "      <td>['Dessert restaurant']</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2023-12-05</td>\n",
       "      <td>Jay</td>\n",
       "      <td>Dessert Heaven in Watford - 5 Stars!</td>\n",
       "      <td>Haute Dolci is a luxury dessert brand that off...</td>\n",
       "      <td>Haute Dolci in Watford is an absolute gem! If ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3896 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating               Restaurant Name  \\\n",
       "0          5                      Buyagift   \n",
       "1          3                      Buyagift   \n",
       "2          5                      Buyagift   \n",
       "3          4                      Buyagift   \n",
       "4          5                      Buyagift   \n",
       "...      ...                           ...   \n",
       "3891       5  Langstone Manor Holiday Park   \n",
       "3892       5                   DickHartley   \n",
       "3893       5          The Beachfield Hotel   \n",
       "3894       5              Crofton and Park   \n",
       "3895       5           Haute Dolci Watford   \n",
       "\n",
       "                                             Categories  Average Rating  \\\n",
       "0     ['Hotel', 'Adventure Sports', 'Racetrack', 'Co...             3.8   \n",
       "1     ['Hotel', 'Adventure Sports', 'Racetrack', 'Co...             3.8   \n",
       "2     ['Hotel', 'Adventure Sports', 'Racetrack', 'Co...             3.8   \n",
       "3     ['Hotel', 'Adventure Sports', 'Racetrack', 'Co...             3.8   \n",
       "4     ['Hotel', 'Adventure Sports', 'Racetrack', 'Co...             3.8   \n",
       "...                                                 ...             ...   \n",
       "3891                                   ['Camping Shop']             3.7   \n",
       "3892                                     ['Restaurant']             3.7   \n",
       "3893                                   ['Family Hotel']             3.7   \n",
       "3894                       ['Jewellery Repair Service']             3.7   \n",
       "3895                             ['Dessert restaurant']             3.7   \n",
       "\n",
       "     Experience Date               Username  \\\n",
       "0         2024-01-09       Ms Cherith Adams   \n",
       "1         2024-01-07         Mr Paul Parker   \n",
       "2         2024-01-09                  Gerri   \n",
       "3         2024-01-09        pPenny Thornton   \n",
       "4         2024-01-09        Adrienne Ralphs   \n",
       "...              ...                    ...   \n",
       "3891      2022-07-09             Liam Perry   \n",
       "3892      2022-06-16                   John   \n",
       "3893      2023-05-18        Bad Experience.   \n",
       "3894      2023-10-25  Poppy Khaisee-Headley   \n",
       "3895      2023-12-05                    Jay   \n",
       "\n",
       "                                           Review Title  \\\n",
       "0                       Amazing offer on a spa retreat…   \n",
       "1                Bought vouchers for a family birthday…   \n",
       "2                          Amazing Cream Tea Experience   \n",
       "3     My choice was not available but the BAG team  ...   \n",
       "4                     I bought a tank drive for a 50th…   \n",
       "...                                                 ...   \n",
       "3891                                 Highly recommended   \n",
       "3892                          Good to do business with.   \n",
       "3893             Clean ,competitive pricing,great food…   \n",
       "3894                 Excellent and professional service   \n",
       "3895               Dessert Heaven in Watford - 5 Stars!   \n",
       "\n",
       "                                            Information  \\\n",
       "0     Buyagift is the UK's leading provider of gift ...   \n",
       "1     Buyagift is the UK's leading provider of gift ...   \n",
       "2     Buyagift is the UK's leading provider of gift ...   \n",
       "3     Buyagift is the UK's leading provider of gift ...   \n",
       "4     Buyagift is the UK's leading provider of gift ...   \n",
       "...                                                 ...   \n",
       "3891  Set amongst delightful mature grounds in a she...   \n",
       "3892  Dick Hartley is an independent supplier of kit...   \n",
       "3893  Welcome to Beachfield Hotel. We're a boutique ...   \n",
       "3894  A premium,personal concierge service.\\r\\n\\r\\nA...   \n",
       "3895  Haute Dolci is a luxury dessert brand that off...   \n",
       "\n",
       "                                            Review Text  \n",
       "0     I got an amazing offer on a spa retreat and th...  \n",
       "1     Bought vouchers for a family birthday present ...  \n",
       "2     A home delivery of our Cream Tea Experience fr...  \n",
       "3     My experience was not available when I tried t...  \n",
       "4     I bought a tank drive for a 50th birthday pres...  \n",
       "...                                                 ...  \n",
       "3891  We had a fantastic time here recently having u...  \n",
       "3892  Good to do business with.Bought some Riedel gl...  \n",
       "3893  Clean ,competitive pricing,great food and staf...  \n",
       "3894  Excellent and professional service. I had a di...  \n",
       "3895  Haute Dolci in Watford is an absolute gem! If ...  \n",
       "\n",
       "[3896 rows x 9 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('restaurant_uk_reviews_v3.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating               int64\n",
       "Restaurant Name     object\n",
       "Categories          object\n",
       "Average Rating     float64\n",
       "Experience Date     object\n",
       "Username            object\n",
       "Review Title        object\n",
       "Information         object\n",
       "Review Text         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating               0\n",
       "Restaurant Name      0\n",
       "Categories           0\n",
       "Average Rating       0\n",
       "Experience Date      0\n",
       "Username             0\n",
       "Review Title         0\n",
       "Information        369\n",
       "Review Text        154\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Review Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating               0\n",
       "Restaurant Name      0\n",
       "Categories           0\n",
       "Average Rating       0\n",
       "Experience Date      0\n",
       "Username             0\n",
       "Review Title         0\n",
       "Information        363\n",
       "Review Text          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correction(texte):\n",
    "    blob = TextBlob(texte)\n",
    "    return str(blob.correct())\n",
    "\n",
    "df_correction = df.copy()\n",
    "# Appliquer la correction orthographique à la colonne \"avis_en\"\n",
    "df_correction['Review Text'] = df_correction['Review Text'].apply(correction)\n",
    "df_correction['Review Title'] = df_correction['Review Title'].apply(correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettre en avant les reviews corrigées  !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a summary of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I got an amazing offer on a spa retreat and the place was incredible. The  process with buyagift was good but being completely honest,  it would have been better if I could have linked through buyagift to complete the booking rather than email the place separately to book.  Mainly because it wasn't immediately confirmed. Would definitely use again though absolutely! Thank you so much \n",
      " I got an amazing offer on a spa retreat and the place was incredible. `\n",
      "\n",
      "\n",
      "Bought vouchers for a family birthday present gift. Purchased e voucher all straight forward so far. Marked down score because been bombarded with requests for feedback! Not processed voucher yet so cant comment any further than buying a voucher. Maybe ask for feedback when there is opportunity to provide the full experience \n",
      " Maybe ask for feedback when there is opportunity to provide the full experience `\n",
      "\n",
      "\n",
      "A home delivery of our Cream Tea Experience from Piglet’s Pantry was amazing !! We loved it so much, so fresh & tasty. Having limited mobility this was just perfect, a cream tea at home. \n",
      " A home delivery of our Cream Tea Experience from Piglet’s Pantry was amazing !! `\n",
      "\n",
      "\n",
      "My experience was not available when I tried to book it with the venue but BAG renewed and gave me an extended date so i could choose something else. The venue had stopped accepting the vouchers and were a bit brusque with me I had waited to book until I was well and had been looking forward to the vineyard visit and aftermoon tea experience. However we then booked afternoon tea at Harvey Nichols in London as we were going to the capital on another visit. We enjoyed this very much. \n",
      " My experience was not available when I tried to book it with the venue but BAG renewed and gave me an extended date so i could choose something else. `\n",
      "\n",
      "\n",
      "I bought a tank drive for a 50th birthday present for our son in law- now booked and we’re hoping he’ll enjoy the experience. No problem with the purchase or booking. \n",
      " I bought a tank drive for a 50th birthday present for our son in law- now booked and we’re hoping he’ll enjoy the experience. `\n",
      "\n",
      "\n",
      "Received Buyagift voucher for tickets for an experience finishing in 1 month (experience closing end of January). I upgraded the gift voucher to a meal for 2, but the expiry date did not change.  If the expiry date had been extended to 6 or 12 months I would have awarded 5 stars. \n",
      " If the expiry date had been extended to 6 or 12 months I would have awarded 5 stars. `\n",
      "\n",
      "\n",
      "I needed some help is extending my vouchers due to my child's ill health, Catriona was extremely patient and helpful and provided excellent customer service.  Thank you so much. \n",
      " I needed some help is extending my vouchers due to my child's ill health, Catriona was extremely patient and helpful and provided excellent customer service. `\n",
      "\n",
      "\n",
      "Difficult to use, hard to search experiences. When I booked I didn’t get much information in the confirmation email (no check in time). Also the hotel didn’t send a separate confirmation so I was worried about whether the booking went through. I will ask my family not to use this website to buy me a gift as it’s not user friendly at all. \n",
      " I will ask my family not to use this website to buy me a gift as it’s not user friendly at all. `\n",
      "\n",
      "\n",
      "DO NOT USE !!!Absolutely useles company . Paid for HelloFresh subscription  voucher and  they charged me our my bank account twice,  spent 5 hours of my.life in total live chatting to Buyagift.  On.2 separate occasions,  fobbed off said would sort and haven't . !!!!!! \n",
      " Paid for HelloFresh subscription  voucher and  they charged me our my bank account twice,  spent 5 hours of my.life in total live chatting to Buyagift. `\n",
      "\n",
      "\n",
      "We were bought a Buyagift voucher for Christmas for a date night for me and my husband. We loved the venue we chose and it was nice to spend some quality time together. I think my only criticism of these vouchers is that it would be good if a few more places let you use them, choice is a little limited \n",
      " I think my only criticism of these vouchers is that it would be good if a few more places let you use them, choice is a little limited `\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_resume(texte):\n",
    "    parser = PlaintextParser.from_string(texte, Tokenizer('english'))\n",
    "    summarizer = LsaSummarizer()\n",
    "    resume = summarizer(parser.document, 1) # Making a resume of one sentence\n",
    "    return \" \".join(str(sentence) for sentence in resume)\n",
    "\n",
    "df['Summary'] = df['Review Text'].apply(generate_resume)\n",
    "\n",
    "for i in range(10):\n",
    "    print(df['Review Text'][i],'\\n', df['Summary'][i],'`\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling negation by replacing words with their anonyms and encountering negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle negation using WordNet\n",
    "def handle_negation(sentence):\n",
    "    result_sentence = list(sentence)\n",
    "    \n",
    "    for i in range(len(result_sentence)):\n",
    "        if result_sentence[i - 1] in ['not', \"n't\"]:\n",
    "            antonyms = []\n",
    "            for syn in wordnet.synsets(result_sentence[i]):\n",
    "                for l in syn.lemmas():\n",
    "                    if l.antonyms():\n",
    "                        antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "            if antonyms:\n",
    "                result_sentence[i] = antonyms[0]\n",
    "\n",
    "    result_sentence = [word for word in result_sentence if word != '']\n",
    "    return result_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing function : Converting emojis, tokenization, lower case, removing punctuation, special characters, numbers and Stop words, handling negation, correction and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "    # Remove numbers\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    # # Handle negation\n",
    "    # tokens = handle_negation(tokens)\n",
    "    \n",
    "    # # Correction \n",
    "    # blob = TextBlob(' '.join(tokens))\n",
    "    # tokens = blob.correct().words\n",
    "     \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # # Stemming \n",
    "    # stemmer = PorterStemmer()\n",
    "    # tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the text processing function to the specified columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Processed Review Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I got an amazing offer on a spa retreat and th...</td>\n",
       "      <td>got amaz offer sea retreat place incred proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bought vouchers for a family birthday present ...</td>\n",
       "      <td>bought voucher famili birthday present gift pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A home delivery of our Cream Tea Experience fr...</td>\n",
       "      <td>home deliveri cream tea experi paget pantri am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My experience was not available when I tried t...</td>\n",
       "      <td>experi avail tri book venu bag renew gave exte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought a tank drive for a 50th birthday pres...</td>\n",
       "      <td>bought tank drive birthday present son book ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Received Buyagift voucher for tickets for an e...</td>\n",
       "      <td>receiv buyagift voucher ticket experi finish m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I needed some help is extending my vouchers du...</td>\n",
       "      <td>need help extend voucher due child ill health ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Difficult to use, hard to search experiences. ...</td>\n",
       "      <td>difficult use hard search experi book get much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DO NOT USE !!!Absolutely useles company . Paid...</td>\n",
       "      <td>use absolut useless compani paid hellofresh su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We were bought a Buyagift voucher for Christma...</td>\n",
       "      <td>bought buyagift voucher christma date night hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Constant issues with the website. Not the firs...</td>\n",
       "      <td>constant issu webster first time issu tri rede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Not worth the money I ended up paying to upgra...</td>\n",
       "      <td>worthless money end pay upgrad extend twice di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Great day out,culture,history and a few drinks...</td>\n",
       "      <td>great day cultur histori drink along way wonde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>We had a wonderful night out with overnight st...</td>\n",
       "      <td>wonder night overnight stay great fun abl talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>My husband bought me a voucher for a night awa...</td>\n",
       "      <td>husband bought voucher night away even meal vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>It’s a gift for my sisters 50th birthday. So e...</td>\n",
       "      <td>gift sister birthday excit see face open actua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I chose the gift box to be sent for my uncles ...</td>\n",
       "      <td>chose gift box sent uncl birthday arriv purcha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Recently bought an afternoon tea voucher as a ...</td>\n",
       "      <td>recent bought afternoon tea voucher birthday g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>It was so relaxing where we stayed and the who...</td>\n",
       "      <td>relax stay whole process book start finish eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The experience vouchers are easy to buy and a ...</td>\n",
       "      <td>experi voucher easi buy good choic activ initi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I was given a voucher for afternoon tea and a ...</td>\n",
       "      <td>given voucher afternoon tea sea experi two cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Treatments were excellent however having to pa...</td>\n",
       "      <td>treatment excel howev pay use towel plu deposi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Experience was a little disappointing, as the ...</td>\n",
       "      <td>experi littl disappoint sea day exclud towel c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ordered the Happy Birthday Experience Box on S...</td>\n",
       "      <td>order happi birthday experi box sunday deliv w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I felt somewhat misled as I bought a gift for ...</td>\n",
       "      <td>felt somewhat misl bought gift famili member e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>My voucher for the Mayfair Grill and Bar was g...</td>\n",
       "      <td>voucher affair drill bar great valu money whol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Brought a afternoon tea voucher last year. Boo...</td>\n",
       "      <td>brought afternoon tea voucher last year book t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I have tried to book a glamping holiday but no...</td>\n",
       "      <td>tri book clamp holiday none site area date sui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Bought a box using the location filter for Nor...</td>\n",
       "      <td>bought box use locat filter northern ireland s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Contacted live chat twice and despite promises...</td>\n",
       "      <td>contract live chat twice despit promis still s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Review Text  \\\n",
       "0   I got an amazing offer on a spa retreat and th...   \n",
       "1   Bought vouchers for a family birthday present ...   \n",
       "2   A home delivery of our Cream Tea Experience fr...   \n",
       "3   My experience was not available when I tried t...   \n",
       "4   I bought a tank drive for a 50th birthday pres...   \n",
       "5   Received Buyagift voucher for tickets for an e...   \n",
       "6   I needed some help is extending my vouchers du...   \n",
       "7   Difficult to use, hard to search experiences. ...   \n",
       "8   DO NOT USE !!!Absolutely useles company . Paid...   \n",
       "9   We were bought a Buyagift voucher for Christma...   \n",
       "10  Constant issues with the website. Not the firs...   \n",
       "11  Not worth the money I ended up paying to upgra...   \n",
       "12  Great day out,culture,history and a few drinks...   \n",
       "13  We had a wonderful night out with overnight st...   \n",
       "14  My husband bought me a voucher for a night awa...   \n",
       "15  It’s a gift for my sisters 50th birthday. So e...   \n",
       "16  I chose the gift box to be sent for my uncles ...   \n",
       "17  Recently bought an afternoon tea voucher as a ...   \n",
       "18  It was so relaxing where we stayed and the who...   \n",
       "19  The experience vouchers are easy to buy and a ...   \n",
       "20  I was given a voucher for afternoon tea and a ...   \n",
       "21  Treatments were excellent however having to pa...   \n",
       "22  Experience was a little disappointing, as the ...   \n",
       "23  Ordered the Happy Birthday Experience Box on S...   \n",
       "24  I felt somewhat misled as I bought a gift for ...   \n",
       "25  My voucher for the Mayfair Grill and Bar was g...   \n",
       "26  Brought a afternoon tea voucher last year. Boo...   \n",
       "27  I have tried to book a glamping holiday but no...   \n",
       "28  Bought a box using the location filter for Nor...   \n",
       "29  Contacted live chat twice and despite promises...   \n",
       "\n",
       "                                Processed Review Text  \n",
       "0   got amaz offer sea retreat place incred proces...  \n",
       "1   bought voucher famili birthday present gift pu...  \n",
       "2   home deliveri cream tea experi paget pantri am...  \n",
       "3   experi avail tri book venu bag renew gave exte...  \n",
       "4   bought tank drive birthday present son book ho...  \n",
       "5   receiv buyagift voucher ticket experi finish m...  \n",
       "6   need help extend voucher due child ill health ...  \n",
       "7   difficult use hard search experi book get much...  \n",
       "8   use absolut useless compani paid hellofresh su...  \n",
       "9   bought buyagift voucher christma date night hu...  \n",
       "10  constant issu webster first time issu tri rede...  \n",
       "11  worthless money end pay upgrad extend twice di...  \n",
       "12  great day cultur histori drink along way wonde...  \n",
       "13  wonder night overnight stay great fun abl talk...  \n",
       "14  husband bought voucher night away even meal vo...  \n",
       "15  gift sister birthday excit see face open actua...  \n",
       "16  chose gift box sent uncl birthday arriv purcha...  \n",
       "17  recent bought afternoon tea voucher birthday g...  \n",
       "18  relax stay whole process book start finish eas...  \n",
       "19  experi voucher easi buy good choic activ initi...  \n",
       "20  given voucher afternoon tea sea experi two cam...  \n",
       "21  treatment excel howev pay use towel plu deposi...  \n",
       "22  experi littl disappoint sea day exclud towel c...  \n",
       "23  order happi birthday experi box sunday deliv w...  \n",
       "24  felt somewhat misl bought gift famili member e...  \n",
       "25  voucher affair drill bar great valu money whol...  \n",
       "26  brought afternoon tea voucher last year book t...  \n",
       "27  tri book clamp holiday none site area date sui...  \n",
       "28  bought box use locat filter northern ireland s...  \n",
       "29  contract live chat twice despit promis still s...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_process = ['Restaurant Name', 'Review Title', 'Information', 'Review Text', 'Summary']\n",
    "\n",
    "df_processed =df_sub.copy()\n",
    "for column in columns_to_process:\n",
    "    df_processed[column] = df_processed[column].astype(str)\n",
    "    new_column_name = 'Processed ' + column \n",
    "    df_processed[new_column_name] = df_processed[column].copy().apply(process_text)\n",
    "\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots Unigrammes Fréquents:\n",
      "[('voucher', 38), ('experi', 23), ('book', 18), ('use', 12), ('would', 10), ('gift', 10), ('buyagift', 9), ('date', 9), ('sea', 8), ('bought', 8)]\n",
      "\n",
      "Bigrammes Fréquents:\n",
      "[('afternoon tea', 5), ('custom servic', 4), ('sea day', 4), ('bought voucher', 3), ('expir date', 3), ('would definit', 2), ('thank much', 2), ('much bought', 2), ('birthday present', 2), ('buy voucher', 2)]\n",
      "\n",
      "Trigrammes Fréquents:\n",
      "[('afternoon tea voucher', 2), ('got amaz offer', 1), ('amaz offer sea', 1), ('offer sea retreat', 1), ('sea retreat place', 1), ('retreat place incred', 1), ('place incred process', 1), ('incred process buyagift', 1), ('process buyagift good', 1), ('buyagift good complet', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour extraire les n-grammes d'une chaîne de texte sans ponctuation et stop words\n",
    "def extraire_ngram(texte, n):\n",
    "    tokens = word_tokenize(texte)\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "# Appliquer la fonction d'extraction des n-grammes et calculer la fréquence des mots sans ponctuation et stop words\n",
    "reviews = ' '.join(df_processed['Processed Review Text'])\n",
    "unigram_freq = FreqDist([mot for mot in word_tokenize(reviews) if mot.isalnum() and mot.lower() not in stopwords.words('english')])\n",
    "bigram_freq = FreqDist(extraire_ngram(reviews, 2))\n",
    "trigram_freq = FreqDist(extraire_ngram(reviews, 3))\n",
    "\n",
    "# Afficher les mots unigrammes et bigrammes les plus fréquents\n",
    "print(\"Mots Unigrammes Fréquents:\")\n",
    "print(unigram_freq.most_common(10))\n",
    "\n",
    "print(\"\\nBigrammes Fréquents:\")\n",
    "print(bigram_freq.most_common(10))\n",
    "\n",
    "print(\"\\nTrigrammes Fréquents:\")\n",
    "print(trigram_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read your DataFrame with cleaned reviews\n",
    "# Assuming you have a DataFrame df with a column 'Cleaned_Avis'\n",
    "# df = pd.read_csv('your_cleaned_dataset.csv')\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "df['Tokens'] = df['Review Text'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Remove rows with empty token lists (NaN values in 'Cleaned_Avis')\n",
    "df = df[df['Tokens'].apply(len) > 0]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(df['Tokens'])\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df['Tokens']]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow import summary\n",
    "\n",
    "# Download a pre-trained spaCy model with GloVe embeddings\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "# Tokenize and lemmatize using spaCy\n",
    "df['Tokens'] = df['Review Text'].apply(lambda x: [token.lemma_ for token in nlp(x)])\n",
    "\n",
    "# Word2Vec training\n",
    "word2vec_model = Word2Vec(sentences=df['Tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# GloVe embeddings from spaCy\n",
    "glove_embeddings = np.array([nlp(word).vector for word in word2vec_model.wv.index_to_key])\n",
    "\n",
    "def visualize_embeddings(embeddings, words, title, top_n=20):\n",
    "    # Sort words based on their frequency/importance\n",
    "    sorted_indices = np.argsort(np.linalg.norm(embeddings, axis=1))[::-1][:top_n]\n",
    "\n",
    "    # Subset embeddings and words\n",
    "    subset_embeddings = embeddings[sorted_indices]\n",
    "    subset_words = [words[i] for i in sorted_indices]\n",
    "\n",
    "    # Perform t-SNE on the subset with a lower perplexity value\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(subset_embeddings)-1))\n",
    "    embeddings_2d = tsne.fit_transform(subset_embeddings)\n",
    "\n",
    "    # Plot the subset\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], marker='o', s=30)\n",
    "\n",
    "    for i, word in enumerate(subset_words):\n",
    "        plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]), xytext=(5, 2),\n",
    "                     textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a subset of the top 20 Word2Vec embeddings\n",
    "visualize_embeddings(word2vec_model.wv.vectors, word2vec_model.wv.index_to_key, 'Word2Vec Embeddings (Top 20)')\n",
    "\n",
    "# Visualize a subset of the top 20 GloVe embeddings\n",
    "visualize_embeddings(glove_embeddings, word2vec_model.wv.index_to_key, 'GloVe Embeddings (Top 20)')\n",
    "\n",
    "\n",
    "# Semantic search using cosine similarity\n",
    "def semantic_search(query, embeddings, words, vector_size, top_k=5):\n",
    "    query_embedding = nlp(query).vector[:vector_size]\n",
    "    embeddings = embeddings[:, :vector_size]\n",
    "    similarity_scores = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    top_indices = np.argsort(similarity_scores)[::-1][:top_k]\n",
    "    return [(words[i], similarity_scores[i]) for i in top_indices]\n",
    "\n",
    "# Example semantic search using Word2Vec embeddings\n",
    "query = \"great product\"\n",
    "results_word2vec = semantic_search(query, word2vec_model.wv.vectors, word2vec_model.wv.index_to_key, vector_size=100)\n",
    "print(f\"Semantic Search Results (Word2Vec) for '{query}':\")\n",
    "for result in results_word2vec:\n",
    "    print(result)\n",
    "\n",
    "# Example semantic search using GloVe embeddings\n",
    "results_glove = semantic_search(query, glove_embeddings, word2vec_model.wv.index_to_key, vector_size=100)\n",
    "print(f\"\\nSemantic Search Results (GloVe) for '{query}':\")\n",
    "for result in results_glove:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
